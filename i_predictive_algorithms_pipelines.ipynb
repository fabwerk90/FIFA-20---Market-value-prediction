{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Preparations for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T17:20:22.379329Z",
     "start_time": "2020-05-07T17:20:22.374315Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:27:48.263402Z",
     "start_time": "2020-05-07T16:27:48.060970Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fp = pd.read_csv(\"data/fp_data_final.csv\", index_col=0)\n",
    "df_gk = pd.read_csv(\"data/gk_data_final.csv\", index_col=0)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:27:48.269387Z",
     "start_time": "2020-05-07T16:27:48.265397Z"
    }
   },
   "outputs": [],
   "source": [
    "# set random seed for all algos\n",
    "rseed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:27:48.279359Z",
     "start_time": "2020-05-07T16:27:48.271381Z"
    }
   },
   "outputs": [],
   "source": [
    "# suppress future warnings for xgboost in this notebook\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare datasets for prediction | general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:27:48.294319Z",
     "start_time": "2020-05-07T16:27:48.281354Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop irrelevant rows\n",
    "\n",
    "df_fp = df_fp.drop([\"player_name\",\"long_name\",\"year_of_birth\",\"height_cm\",\"weight_kg\",\"nationality\",\"club\"],axis=1)\n",
    "\n",
    "df_gk = df_gk.drop([\"player_name\",\"long_name\",\"year_of_birth\",\"height_cm\",\"weight_kg\",\"nationality\",\"club\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:27:49.568389Z",
     "start_time": "2020-05-07T16:27:49.549413Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 1. put final test set aside\n",
    "\n",
    "# fieldplayers\n",
    "train_set_fp = df_fp.sample(frac=0.80, random_state=rseed)\n",
    "final_test_set_fp = df_fp.drop(train_set_fp.index)\n",
    "\n",
    "# goal keepers\n",
    "train_set_gk = df_gk.sample(frac=0.80, random_state=rseed)\n",
    "final_test_set_gk = df_gk.drop(train_set_gk.index)\n",
    "\n",
    "print(\"Fieldplayers: \")\n",
    "print(train_set_fp.shape)\n",
    "print(final_test_set_fp.shape)\n",
    "print(\"-----------------------\")\n",
    "print(\"Goalkeepers: \")\n",
    "print(train_set_gk.shape)\n",
    "print(final_test_set_gk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T17:22:21.141085Z",
     "start_time": "2020-05-07T17:22:21.133109Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preprocessing_pipeline (train_set, scaler, encoder):\n",
    "\n",
    "    ######################################################################\n",
    "    # I. train-test-split\n",
    "    ######################################################################\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = train_set.drop([\"market_value_in_euro\"],axis=1)\n",
    "\n",
    "    y = train_set[\"market_value_in_euro\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rseed)\n",
    "\n",
    "    ###################################\n",
    "    # II. Build preprocessing pipeline with\n",
    "    # - a scaler for numerical columns\n",
    "    # - an encoder for categorical columns\n",
    "    # --> scaler and encoder need to be imported before function is called\n",
    "    ###################################\n",
    "\n",
    "    from sklearn.pipeline import Pipeline  \n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', scaler)])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('encoder', encoder)])\n",
    "\n",
    "    ######################################################################\n",
    "    # III. Apply column transformer\n",
    "    ######################################################################\n",
    "\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "\n",
    "    numerical_features = train_set.select_dtypes(include=['int64', 'float64']).drop([\"market_value_in_euro\"],axis=1).columns\n",
    "    categorical_features = train_set.select_dtypes(include=['object']).columns\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numerical_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features)])\n",
    "    \n",
    "    X_pre = preprocessor.fit_transform(X)\n",
    "    \n",
    "    ######################################################################\n",
    "    # IV. Extract column names for feature importances later on\n",
    "    ######################################################################\n",
    "    \n",
    "    \n",
    "    numerical_features_names = list(numerical_features)\n",
    "    encoded_categorical_features = preprocessor.named_transformers_[\"cat\"][\"encoder\"]\n",
    "    encoded_categorical_features_names = list(encoded_categorical_features.get_feature_names(categorical_features))\n",
    "    column_names = numerical_features_names + encoded_categorical_features_names\n",
    "\n",
    "    return X, y, X_train, X_test, y_train, y_test, column_names, preprocessor  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Pipeline\n",
    "\n",
    "Regressors to be tested:\n",
    "1. Linear Regression\n",
    "6. Stochastic Gradient Descent\n",
    "7. Decision Trees\n",
    "8. Random Forest\n",
    "9. AdaBoost\n",
    "10. Gradient Tree Boosting\n",
    "11. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:55:42.462394Z",
     "start_time": "2020-05-07T16:55:42.448431Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def model_selection_pipeline (preprocessor_pipeline, scoring=\"r2\"):\n",
    "    \n",
    "    # get processed data from preprocessing pipeline\n",
    "    X, y, X_train, X_test, y_train, y_test, column_names, preprocessor = preprocessor_pipeline\n",
    "\n",
    "    ######################################################################\n",
    "    # I. Define and import relevant algorithms\n",
    "    ######################################################################\n",
    "\n",
    "    from sklearn import linear_model\n",
    "    from sklearn import svm\n",
    "    from sklearn import tree\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    import xgboost as xgb\n",
    "    \n",
    "\n",
    "    models = []\n",
    "    models.append((\"Linear Regression\",linear_model.LinearRegression()))\n",
    "    models.append((\"SGD\",linear_model.SGDRegressor(random_state=rseed)))\n",
    "    models.append((\"Decision Tree\",tree.DecisionTreeRegressor(random_state=rseed)))\n",
    "    models.append((\"Random Forest\",RandomForestRegressor(random_state=rseed,n_jobs=-1)))\n",
    "    models.append((\"AdaBoost\",AdaBoostRegressor(random_state=rseed)))\n",
    "    models.append((\"GradientBoosting\",GradientBoostingRegressor(random_state=rseed)))\n",
    "    models.append((\"XGBoost\",xgb.XGBRegressor(random_state=rseed)))\n",
    "    \n",
    "\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = scoring\n",
    "    \n",
    "    ######################################################################\n",
    "    # II. Fit models and calculate metric for each model\n",
    "    ######################################################################\n",
    "    \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    from sklearn import model_selection\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    import math\n",
    "    \n",
    "    for name,regressor in models:\n",
    "        \n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('regressor', regressor)])\n",
    "        \n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        kfold = model_selection.KFold(n_splits=5, random_state=rseed, shuffle=True)\n",
    "        cv_results = abs(model_selection.cross_val_score(pipe, X, y, cv=kfold, scoring=scoring))\n",
    "        cv_mean = math.ceil(cv_results.mean())\n",
    "        cv_std = math.ceil(cv_results.std())\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        from yellowbrick.regressor import residuals_plot\n",
    "        from yellowbrick.datasets import load_concrete\n",
    "        \n",
    "        print(f\"Residual plot for {name}\")\n",
    "        viz = residuals_plot(pipe, X_train, y_train, X_test, y_test, hist=False)\n",
    "\n",
    "    # boxplot algorithm comparison\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:57:47.586841Z",
     "start_time": "2020-05-07T16:56:07.703099Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_selection_pipeline (preprocessing_pipeline(train_set_fp, MinMaxScaler(), OneHotEncoder()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --> Best 3 performing algorithms (in terms of R-squared) are Random Forest, GradientBoost and XGBoost!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearchCV Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "narrow down, where to look for with gridsearch afterward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T17:26:38.925111Z",
     "start_time": "2020-05-07T17:26:38.914114Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def randomsearch_pipeline(preprocessor_pipeline, model_name, model, param_grid, scoring=\"r2\"):\n",
    "    \n",
    "    # model needs to be a tuple of \"(name, regressor)\"\n",
    "    \n",
    "    # get processed data from preprocessing pipeline\n",
    "    X, y, X_train, X_test, y_train, y_test, column_names, preprocessor = preprocessor_pipeline\n",
    "\n",
    "    ###################################\n",
    "    # I. \n",
    "    ###################################\n",
    "    \n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    \n",
    "    param_grid = param_grid\n",
    "    \n",
    "\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)])\n",
    "\n",
    "\n",
    "    grid = RandomizedSearchCV(pipe, param_grid, cv=5, scoring=scoring, random_state=rseed, n_jobs=-1)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = grid.predict(X_test)\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"\\033[1m\",model_name,\"\\033[0m\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Best estimator score:\")\n",
    "    print(grid.best_score_)\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "    # features importances\n",
    "\n",
    "    feature_importances = grid.best_estimator_.named_steps[\"regressor\"].feature_importances_\n",
    "    feat_imp_df = pd.DataFrame({'feature':column_names,'importance':feature_importances})\n",
    "    feat_imp_df = feat_imp_df.sort_values(by='importance', ascending=False)\n",
    "    print(\"Feature importances:\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(feat_imp_df.head(20))\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    # Visualization of residuals\n",
    "\n",
    "    from yellowbrick.regressor import residuals_plot\n",
    "    from yellowbrick.datasets import load_concrete\n",
    "\n",
    "    print(f\"Residual plot for {model_name}\")\n",
    "    viz = residuals_plot(grid, X_train, y_train, X_test, y_test, hist=False)\n",
    "    \n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T17:28:20.397696Z",
     "start_time": "2020-05-07T17:26:39.197767Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid ={\n",
    "'model__bootstrap': [True, False],\n",
    "'model__max_depth': [1,10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "'model__max_features': ['auto', 'sqrt'],\n",
    "'model__min_samples_leaf': [1, 2, 4, 6],\n",
    "'model__min_samples_split': [2, 5, 10],\n",
    "'model__n_estimators': [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "randomsearch_pipeline (preprocessing_pipeline(train_set_fp, MinMaxScaler(), OneHotEncoder()),\"Random Forest\",RandomForestRegressor(random_state=rseed,n_jobs=-1),param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Pipeline (with optimal hyperparameters from GridSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stacking model function will be defined here for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:13:01.886414Z",
     "start_time": "2020-05-07T16:13:01.867463Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# stacking model for later\n",
    "\n",
    "### need to give in list with tuples of (name, regressor)\n",
    "### can be given from algo function above\n",
    "\n",
    "    from sklearn.ensemble import StackingRegressor\n",
    "    \n",
    "    stacked_model = StackingRegressor(\n",
    "        estimators = fitted_models,\n",
    "        final_estimator=RandomForestRegressor(n_estimators=10,random_state=rseed)\n",
    "    )\n",
    "    \n",
    "    stacked_model.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:37:32.887256Z",
     "start_time": "2020-05-07T14:37:32.501262Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=\"league\", y=\"market_value_in_euro\", data=df_fp)\n",
    "plt.xticks(rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T13:56:49.032444Z",
     "start_time": "2020-05-07T13:54:16.128144Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "###################################\n",
    "# 2.1. train-test-split with remaining 80% of data\n",
    "###################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_set_fp.drop([\"market_value_in_euro\"],axis=1)\n",
    "\n",
    "y = train_set_fp[\"market_value_in_euro\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rseed)\n",
    "\n",
    "###################################\n",
    "# 2.2. Build preprocessing pipeline with\n",
    "# - a scaler for numerical columns\n",
    "# - an encoder for categorical columns\n",
    "###################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder())])\n",
    "\n",
    "###################################\n",
    "# 2.3. Apply column transformer\n",
    "###################################\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_features = train_set_fp.select_dtypes(include=['int64', 'float64']).drop([\"market_value_in_euro\"],axis=1).columns\n",
    "categorical_features = train_set_fp.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)])\n",
    "\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# get column names for feature importances later\n",
    "\n",
    "numerical_features_names = list(numerical_features)\n",
    "encoded_categorical_features = preprocessor.named_transformers_[\"cat\"][\"encoder\"]\n",
    "encoded_categorical_features_names = list(encoded_categorical_features.get_feature_names(categorical_features))\n",
    "column_names = numerical_features_names + encoded_categorical_features_names\n",
    "\n",
    "###################################\n",
    "# 2.4. Fit algorithm\n",
    "###################################\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [50, 100, 200, 250],\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'regressor__max_depth' : [3,4,5,12,20,30]\n",
    "}\n",
    "\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                  ('regressor', xgb.XGBRegressor(random_state=rseed))])\n",
    "\n",
    "grid = RandomizedSearchCV(model, param_grid, cv=5, scoring=\"r2\", random_state=rseed, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = grid.predict(X_train)\n",
    "y_pred_test = grid.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Best hyperparameters:\")\n",
    "print(grid.best_params_)\n",
    "print(\"-----------------------\")\n",
    "print(\"Best estimator score:\")\n",
    "print(grid.best_score_)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# features importances\n",
    "\n",
    "feature_importances = grid.best_estimator_.named_steps[\"regressor\"].feature_importances_\n",
    "feat_imp_df = pd.DataFrame({'feature':column_names,'importance':feature_importances})\n",
    "feat_imp_df = feat_imp_df.sort_values(by='importance', ascending=False)\n",
    "print(\"Feature importances:\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(feat_imp_df.head(20))\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "# Visualization of residuals\n",
    "\n",
    "from yellowbrick.regressor import residuals_plot\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "viz = residuals_plot(grid, X_train, y_train, X_test, y_test, hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T13:58:38.789248Z",
     "start_time": "2020-05-07T13:58:38.756309Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:06:16.367205Z",
     "start_time": "2020-05-07T14:06:16.347259Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# put together datasets again\n",
    "df_fp_train = X_train.copy()\n",
    "df_fp_train[\"y_actual\"] = y_train\n",
    "df_fp_train[\"y_predicted\"] = y_pred_train\n",
    "\n",
    "df_fp_test = X_test.copy()\n",
    "df_fp_test[\"y_actual\"] = y_test\n",
    "df_fp_test[\"y_predicted\"] = y_pred_test\n",
    "\n",
    "# calculate residuals\n",
    "df_fp_train[\"residuals\"] = df_fp_train[\"y_actual\"] - df_fp_train[\"y_predicted\"]\n",
    "df_fp_test[\"residuals\"] = df_fp_test[\"y_actual\"] - df_fp_test[\"y_predicted\"]\n",
    "\n",
    "\n",
    "# calculate R2 score\n",
    "ssr_train = np.sum((df_fp_train[\"y_predicted\"] - df_fp_train[\"y_actual\"])**2)\n",
    "ssr_test = np.sum((df_fp_test[\"y_predicted\"] - df_fp_test[\"y_actual\"])**2)\n",
    "\n",
    "sst_train = np.sum((df_fp_train[\"y_actual\"] - np.mean(df_fp_train[\"y_actual\"]))**2)\n",
    "sst_test = np.sum((df_fp_test[\"y_actual\"] - np.mean(df_fp_test[\"y_actual\"]))**2)\n",
    "\n",
    "r2_score_train = 1 - (ssr_train/sst_train)\n",
    "r2_score_test = 1 - (ssr_test/sst_test)\n",
    "\n",
    "print(round(r2_score_train,3))\n",
    "print(round(r2_score_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:44:37.370045Z",
     "start_time": "2020-05-07T14:44:36.873375Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "ax = sns.scatterplot(x=\"y_predicted\", y=\"residuals\", hue=\"overall\",data=df_fp_train, \n",
    "                     legend=False, palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:34:30.041743Z",
     "start_time": "2020-05-07T14:34:29.575991Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "ax = sns.scatterplot(x=\"y_predicted\", y=\"residuals\", hue=\"main_position\",data=df_fp_train, \n",
    "                     legend=False, palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:28:18.770109Z",
     "start_time": "2020-05-07T14:28:14.861223Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize residuals\n",
    "plt.style.use(\"seaborn\")\n",
    "norm_resids = [float(i)/sum(df_fp_train[\"residuals\"]) for i in df_fp_train[\"residuals\"]]\n",
    "sns.distplot(norm_resids, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDos\n",
    "- diagonal structure in residuals? --> googlen\n",
    "- weitere regressionsmodelle checken (linear u.a.), ob die Residual-Struktur von RF, GRD, XGB auch für diese gilt\n",
    "- alle categoricals checken, so wie für die beiden residual plots oben\n",
    "- Prozentualer fehler-abweichung genauso wie die residuals normalisieren und als distplot\n",
    "- irgendwas falsch in pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todos after 2'\n",
    "\n",
    "- Feature selection / Recursive elimination mit feature importances\n",
    "- Model selection(k-best) anschauen (von dirk?)\n",
    "\n",
    "- richtige GridSearch vor der Pipeline\n",
    "- Stacking Pipeline mit den Gridparametern\n",
    "- Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
